# 🖥️ 本地加载模型方案

## ✅ 已实现

我已经修改了代码，现在系统会**优先使用本地加载的模型**。

## 📊 优先级顺序

1. **本地 Hugging Face 模型**（最佳）
   - 直接从 Hugging Face 下载并加载到本地
   - 不依赖外部 API
   - 准确率最高
   - 首次加载需要下载（约 500MB）

2. **Hugging Face Inference API**（备用）
   - 如果本地加载失败，尝试 API
   - 但你的模型太大，API 可能不可用

3. **关键词分类器**（最后备用）
   - 如果以上都失败，使用关键词匹配

## 🔧 工作原理

### 首次使用

1. **首次调用时**：
   - 从 Hugging Face 下载模型（约 500MB）
   - 加载到内存（约 1-2GB）
   - 缓存模型，后续调用直接使用

2. **后续调用**：
   - 直接使用缓存的模型
   - 速度很快（毫秒级）

### 内存要求

- **模型文件**: 475.5 MB
- **加载后内存**: 约 1-2 GB
- **总需求**: 至少 2 GB 可用内存

## ⚠️ 注意事项

### Streamlit Cloud 限制

Streamlit Cloud 免费版可能：
- 内存不足（通常只有 1GB）
- 无法加载大模型

**解决方案**：
- 使用 Streamlit Cloud Pro（付费）
- 或部署到自己的服务器（有足够内存）

### 本地运行

如果你在本地运行：
- 确保有至少 2GB 可用内存
- 首次加载需要下载模型（需要网络）

## 🚀 使用方法

代码已自动配置，无需额外设置：

1. **确保配置了 HF_TOKEN 和 HF_MODEL**：
   ```toml
   # .streamlit/secrets.toml 或 Streamlit Cloud Secrets
   HF_TOKEN = "your-token"
   HF_MODEL = "zylandy/mae-intent-classifier"
   ```

2. **运行应用**：
   - 首次调用会自动下载并加载模型
   - 后续调用直接使用缓存的模型

3. **查看使用的分类器**：
   - 在应用界面查看意图分类结果
   - 如果看到高置信度（>0.7），说明使用了本地模型

## 🔍 如何确认本地模型已加载

### 方法 1: 查看日志

应用启动时应该看到：
```
🔄 Loading model locally: zylandy/mae-intent-classifier
✅ Model loaded successfully
```

### 方法 2: 查看分类结果

如果使用本地模型：
- 置信度通常较高（>0.7）
- 对复杂句子的分类更准确
- 方法显示为 `hf_local`

## 💡 如果本地加载失败

如果看到错误（如内存不足）：
1. **检查可用内存**：至少需要 2GB
2. **考虑使用 Inference Endpoints**（付费，但更可靠）
3. **或继续使用关键词分类器**（如果准确率可接受）

## 📝 总结

- ✅ **已实现本地加载**：优先使用你的模型
- ✅ **自动缓存**：首次加载后，后续调用很快
- ✅ **智能降级**：如果本地加载失败，自动尝试 API 或关键词分类器
- ⚠️ **内存要求**：至少 2GB 可用内存

**现在你的模型会优先被使用！** 🎉
