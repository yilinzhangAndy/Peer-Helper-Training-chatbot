"""
UF MAE Website Real-time Scraper
å®æ—¶æœç´¢ UF MAE ç½‘ç«™è·å–æœ€æ–°ä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯è¯¾ç¨‹ä¿¡æ¯ï¼‰
"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import re
from urllib.parse import urljoin, quote


class UFMAEWebScraper:
    """å®æ—¶æœç´¢ UF MAE ç½‘ç«™çš„å·¥å…·ç±»"""
    
    BASE_URL = "https://mae.ufl.edu"
    COURSE_SCHEDULE_URLS = {
        "spring": "https://mae.ufl.edu/undergraduate/course-schedules/spring-2025/",
        "summer": "https://mae.ufl.edu/undergraduate/course-schedules/summer-2025/",
        "fall": "https://mae.ufl.edu/undergraduate/course-schedules/fall-2025/"
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def search_course_schedule(self, semester: str = "spring", course_code: Optional[str] = None) -> List[Dict]:
        """
        æœç´¢è¯¾ç¨‹è¡¨ä¿¡æ¯
        
        Args:
            semester: å­¦æœŸ (spring, summer, fall)
            course_code: è¯¾ç¨‹ä»£ç  (å¦‚ "EML2023", "EML3100")ï¼Œå¯é€‰
        
        Returns:
            è¯¾ç¨‹ä¿¡æ¯åˆ—è¡¨
        """
        try:
            url = self.COURSE_SCHEDULE_URLS.get(semester.lower(), self.COURSE_SCHEDULE_URLS["spring"])
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            courses = []
            
            # å°è¯•ä¸åŒçš„è¡¨æ ¼ç»“æ„
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                headers = []
                
                # è·å–è¡¨å¤´
                if rows:
                    header_row = rows[0]
                    headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
                
                # è§£ææ•°æ®è¡Œ
                for row in rows[1:]:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) < 2:
                        continue
                    
                    course_data = {}
                    for i, cell in enumerate(cells):
                        header = headers[i] if i < len(headers) else f"col_{i}"
                        course_data[header] = cell.get_text(strip=True)
                    
                    # å¦‚æœæŒ‡å®šäº†è¯¾ç¨‹ä»£ç ï¼Œè¿›è¡Œè¿‡æ»¤
                    if course_code:
                        course_text = ' '.join(course_data.values()).upper()
                        if course_code.upper() not in course_text:
                            continue
                    
                    if course_data:
                        courses.append(course_data)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼ï¼Œå°è¯•æœç´¢æ–‡æœ¬å†…å®¹
            if not courses:
                page_text = soup.get_text()
                if course_code:
                    # æœç´¢åŒ…å«è¯¾ç¨‹ä»£ç çš„æ®µè½
                    pattern = rf'\b{re.escape(course_code.upper())}\b[^\n]*'
                    matches = re.findall(pattern, page_text, re.IGNORECASE)
                    for match in matches[:5]:  # æœ€å¤šè¿”å›5ä¸ªåŒ¹é…
                        courses.append({"course_info": match.strip()})
            
            return courses[:10]  # æœ€å¤šè¿”å›10ä¸ªç»“æœ
            
        except Exception as e:
            print(f"âš ï¸ Error searching course schedule: {e}")
            return []
    
    def search_website(self, query: str, max_results: int = 5) -> List[str]:
        """
        åœ¨ UF MAE ç½‘ç«™ä¸Šæœç´¢ç›¸å…³ä¿¡æ¯
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            ç›¸å…³æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        results = []
        
        try:
            # æœç´¢è¯¾ç¨‹è¡¨é¡µé¢
            if any(keyword in query.lower() for keyword in ['course', 'class', 'schedule', 'semester', 'spring', 'summer', 'fall']):
                semester = "spring"  # é»˜è®¤æ˜¥å­£å­¦æœŸ
                if "summer" in query.lower():
                    semester = "summer"
                elif "fall" in query.lower():
                    semester = "fall"
                
                # æå–å¯èƒ½çš„è¯¾ç¨‹ä»£ç 
                course_code = None
                course_pattern = r'\b([A-Z]{3}\d{4})\b'
                matches = re.findall(course_pattern, query.upper())
                if matches:
                    course_code = matches[0]
                
                courses = self.search_course_schedule(semester, course_code)
                for course in courses[:max_results]:
                    course_text = " | ".join([f"{k}: {v}" for k, v in course.items() if v])
                    if course_text:
                        results.append(f"Course Schedule: {course_text}")
            
            # æœç´¢ä¸»é¡µé¢
            try:
                response = self.session.get(self.BASE_URL, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æœç´¢åŒ…å«å…³é”®è¯çš„æ–‡æœ¬
                page_text = soup.get_text()
                query_words = query.lower().split()
                
                # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ®µè½
                paragraphs = page_text.split('\n')
                for para in paragraphs:
                    para_lower = para.lower()
                    if any(word in para_lower for word in query_words if len(word) > 2):
                        if len(para.strip()) > 20 and len(para.strip()) < 500:
                            results.append(para.strip())
                            if len(results) >= max_results:
                                break
            except Exception as e:
                print(f"âš ï¸ Error searching main page: {e}")
            
            return results[:max_results]
            
        except Exception as e:
            print(f"âš ï¸ Error in website search: {e}")
            return []
    
    def get_course_info(self, course_code: str, semester: str = "spring") -> Optional[Dict]:
        """
        è·å–ç‰¹å®šè¯¾ç¨‹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            course_code: è¯¾ç¨‹ä»£ç  (å¦‚ "EML2023")
            semester: å­¦æœŸ
        
        Returns:
            è¯¾ç¨‹ä¿¡æ¯å­—å…¸
        """
        courses = self.search_course_schedule(semester, course_code)
        if courses:
            return courses[0]
        return None


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    scraper = UFMAEWebScraper()
    
    print("ğŸ” æµ‹è¯• UF MAE ç½‘ç«™å®æ—¶æœç´¢:")
    print("=" * 60)
    
    # æµ‹è¯•1: æœç´¢è¯¾ç¨‹
    print("\n1. æœç´¢è¯¾ç¨‹ä¿¡æ¯:")
    results = scraper.search_website("EML2023 spring course", max_results=3)
    for i, result in enumerate(results, 1):
        print(f"   {i}. {result[:100]}...")
    
    # æµ‹è¯•2: æœç´¢è¯¾ç¨‹è¡¨
    print("\n2. æœç´¢è¯¾ç¨‹è¡¨:")
    courses = scraper.search_course_schedule("spring", "EML")
    print(f"   æ‰¾åˆ° {len(courses)} é—¨è¯¾ç¨‹")
    for i, course in enumerate(courses[:3], 1):
        print(f"   {i}. {course}")
    
    # æµ‹è¯•3: æœç´¢ä¸€èˆ¬ä¿¡æ¯
    print("\n3. æœç´¢ç ”ç©¶é¢†åŸŸ:")
    results = scraper.search_website("robotics research", max_results=3)
    for i, result in enumerate(results, 1):
        print(f"   {i}. {result[:100]}...")
