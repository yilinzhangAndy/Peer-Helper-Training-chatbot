"""
UF MAE Website Real-time Scraper
å®æ—¶æœç´¢ UF MAE ç½‘ç«™è·å–æœ€æ–°ä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯è¯¾ç¨‹ä¿¡æ¯ï¼‰
"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Set
import re
import time
import json
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote


class UFMAEWebScraper:
    """å®æ—¶æœç´¢ UF MAE ç½‘ç«™çš„å·¥å…·ç±»"""
    
    BASE_URL = "https://mae.ufl.edu"
    COURSE_SCHEDULE_URLS = {
        "spring": "https://mae.ufl.edu/undergraduate/course-schedules/spring-2025/",
        "summer": "https://mae.ufl.edu/undergraduate/course-schedules/summer-2025/",
        "fall": "https://mae.ufl.edu/undergraduate/course-schedules/fall-2025/"
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def search_course_schedule(self, semester: str = "spring", course_code: Optional[str] = None) -> List[Dict]:
        """
        æœç´¢è¯¾ç¨‹è¡¨ä¿¡æ¯
        
        Args:
            semester: å­¦æœŸ (spring, summer, fall)
            course_code: è¯¾ç¨‹ä»£ç  (å¦‚ "EML2023", "EML3100")ï¼Œå¯é€‰
        
        Returns:
            è¯¾ç¨‹ä¿¡æ¯åˆ—è¡¨
        """
        try:
            url = self.COURSE_SCHEDULE_URLS.get(semester.lower(), self.COURSE_SCHEDULE_URLS["spring"])
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            courses = []
            
            # å°è¯•ä¸åŒçš„è¡¨æ ¼ç»“æ„
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                headers = []
                
                # è·å–è¡¨å¤´
                if rows:
                    header_row = rows[0]
                    headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
                
                # è§£ææ•°æ®è¡Œ
                for row in rows[1:]:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) < 2:
                        continue
                    
                    course_data = {}
                    for i, cell in enumerate(cells):
                        header = headers[i] if i < len(headers) else f"col_{i}"
                        course_data[header] = cell.get_text(strip=True)
                    
                    # å¦‚æœæŒ‡å®šäº†è¯¾ç¨‹ä»£ç ï¼Œè¿›è¡Œè¿‡æ»¤
                    if course_code:
                        course_text = ' '.join(course_data.values()).upper()
                        if course_code.upper() not in course_text:
                            continue
                    
                    if course_data:
                        courses.append(course_data)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼ï¼Œå°è¯•æœç´¢æ–‡æœ¬å†…å®¹
            if not courses:
                page_text = soup.get_text()
                if course_code:
                    # æœç´¢åŒ…å«è¯¾ç¨‹ä»£ç çš„æ®µè½
                    pattern = rf'\b{re.escape(course_code.upper())}\b[^\n]*'
                    matches = re.findall(pattern, page_text, re.IGNORECASE)
                    for match in matches[:5]:  # æœ€å¤šè¿”å›5ä¸ªåŒ¹é…
                        courses.append({"course_info": match.strip()})
            
            return courses[:10]  # æœ€å¤šè¿”å›10ä¸ªç»“æœ
            
        except Exception as e:
            print(f"âš ï¸ Error searching course schedule: {e}")
            return []
    
    def search_website(self, query: str, max_results: int = 5) -> List[str]:
        """
        åœ¨ UF MAE ç½‘ç«™ä¸Šæœç´¢ç›¸å…³ä¿¡æ¯
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            ç›¸å…³æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        results = []
        
        try:
            # æœç´¢è¯¾ç¨‹è¡¨é¡µé¢
            if any(keyword in query.lower() for keyword in ['course', 'class', 'schedule', 'semester', 'spring', 'summer', 'fall']):
                semester = "spring"  # é»˜è®¤æ˜¥å­£å­¦æœŸ
                if "summer" in query.lower():
                    semester = "summer"
                elif "fall" in query.lower():
                    semester = "fall"
                
                # æå–å¯èƒ½çš„è¯¾ç¨‹ä»£ç 
                course_code = None
                course_pattern = r'\b([A-Z]{3}\d{4})\b'
                matches = re.findall(course_pattern, query.upper())
                if matches:
                    course_code = matches[0]
                
                courses = self.search_course_schedule(semester, course_code)
                for course in courses[:max_results]:
                    course_text = " | ".join([f"{k}: {v}" for k, v in course.items() if v])
                    if course_text:
                        results.append(f"Course Schedule: {course_text}")
            
            # æœç´¢ä¸»é¡µé¢
            try:
                response = self.session.get(self.BASE_URL, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æœç´¢åŒ…å«å…³é”®è¯çš„æ–‡æœ¬
                page_text = soup.get_text()
                query_words = query.lower().split()
                
                # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ®µè½
                paragraphs = page_text.split('\n')
                for para in paragraphs:
                    para_lower = para.lower()
                    if any(word in para_lower for word in query_words if len(word) > 2):
                        if len(para.strip()) > 20 and len(para.strip()) < 500:
                            results.append(para.strip())
                            if len(results) >= max_results:
                                break
            except Exception as e:
                print(f"âš ï¸ Error searching main page: {e}")
            
            return results[:max_results]
            
        except Exception as e:
            print(f"âš ï¸ Error in website search: {e}")
            return []
    
    def _is_same_domain(self, url: str) -> bool:
        """Check if URL belongs to mae.ufl.edu."""
        parsed = urlparse(url)
        netloc = parsed.netloc.lower()
        return "mae.ufl.edu" in netloc or netloc == "mae.ufl.edu"

    def _normalize_url(self, base: str, href: str) -> Optional[str]:
        """Resolve relative URL and return absolute URL if same domain."""
        if not href or href.startswith(("#", "mailto:", "tel:", "javascript:", "data:")):
            return None
        full = urljoin(base, href)
        full = full.split("#")[0].rstrip("/") or full
        if not full.startswith("http"):
            return None
        return full if self._is_same_domain(full) else None

    def _extract_text(self, soup: BeautifulSoup) -> str:
        """Extract main text, skip nav/footer/script."""
        for tag in soup.find_all(["script", "style", "nav", "footer", "header"]):
            tag.decompose()
        text = soup.get_text(separator="\n")
        lines = [ln.strip() for ln in text.splitlines() if ln.strip() and len(ln.strip()) > 15]
        return "\n".join(lines[:80])  # cap length per page

    def crawl_full_site(
        self,
        start_url: str = None,
        max_pages: int = 150,
        max_depth: int = 5,
        delay_sec: float = 0.6,
    ) -> List[Dict[str, str]]:
        """
        Recursively crawl MAE site (About, People, Undergraduate, Graduate, Research, etc.).
        Returns list of {url, title, content} for knowledge base.
        """
        start_url = start_url or self.BASE_URL
        if not self._is_same_domain(start_url):
            return []
        visited: Set[str] = set()
        queued: Set[str] = {start_url}
        results: List[Dict[str, str]] = []
        queue: List[tuple] = [(start_url, 0)]

        while queue and len(visited) < max_pages:
            url, depth = queue.pop(0)
            if url in visited or depth > max_depth:
                continue
            visited.add(url)
            try:
                resp = self.session.get(url, timeout=15)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.content, "html.parser")
                title = (soup.find("title") or soup.find("h1"))
                title_text = title.get_text(strip=True) if title else ""
                content = self._extract_text(soup)
                if content and len(content) > 30:
                    results.append({
                        "url": url,
                        "title": title_text or url,
                        "content": content[:2000],
                    })
                for a in soup.find_all("a", href=True):
                    next_url = self._normalize_url(url, a["href"])
                    if next_url and next_url not in queued:
                        queued.add(next_url)
                        queue.append((next_url, depth + 1))
                time.sleep(delay_sec)
            except Exception as e:
                print(f"âš ï¸ Skip {url}: {e}")
        return results

    def crawl_and_save_to_json(self, output_path: str = None, **kwargs) -> str:
        """Crawl full site and save to JSON. Returns path to saved file."""
        output_path = output_path or str(Path(__file__).parent / "knowledge_base" / "mae_full_site_knowledge.json")
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        data = self.crawl_full_site(**kwargs)
        out = [{"question": f"{d['title']} ({d['url']})", "answer": d["content"], "source": d["url"]} for d in data]
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False, indent=2)
        return output_path

    def get_course_info(self, course_code: str, semester: str = "spring") -> Optional[Dict]:
        """
        è·å–ç‰¹å®šè¯¾ç¨‹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            course_code: è¯¾ç¨‹ä»£ç  (å¦‚ "EML2023")
            semester: å­¦æœŸ
        
        Returns:
            è¯¾ç¨‹ä¿¡æ¯å­—å…¸
        """
        courses = self.search_course_schedule(semester, course_code)
        if courses:
            return courses[0]
        return None


# æµ‹è¯•ä»£ç  / å…¨ç«™çˆ¬å–
if __name__ == "__main__":
    import sys
    scraper = UFMAEWebScraper()

    if len(sys.argv) > 1 and sys.argv[1] == "crawl":
        print("ğŸ•·ï¸ Crawling full MAE site (About, Undergraduate, Graduate, Research, etc.)...")
        out_path = scraper.crawl_and_save_to_json(max_pages=150, max_depth=5, delay_sec=0.6)
        print(f"âœ… Saved to {out_path}")
    else:
        print("ğŸ” æµ‹è¯• UF MAE ç½‘ç«™å®æ—¶æœç´¢:")
        print("=" * 60)
        print("\n1. æœç´¢è¯¾ç¨‹ä¿¡æ¯:")
        results = scraper.search_website("EML2023 spring course", max_results=3)
        for i, result in enumerate(results, 1):
            print(f"   {i}. {result[:100]}...")
        print("\n2. æœç´¢è¯¾ç¨‹è¡¨:")
        courses = scraper.search_course_schedule("spring", "EML")
        print(f"   æ‰¾åˆ° {len(courses)} é—¨è¯¾ç¨‹")
        print("\n3. æœç´¢ç ”ç©¶é¢†åŸŸ:")
        results = scraper.search_website("robotics research", max_results=3)
        for i, result in enumerate(results, 1):
            print(f"   {i}. {result[:100]}...")
        print("\nğŸ’¡ Run with 'crawl' to crawl full site: python uf_mae_web_scraper.py crawl")
