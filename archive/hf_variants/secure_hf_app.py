import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import os

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="MAE Intent Classifier",
    page_icon="ğŸ“",
    layout="wide"
)

# å®‰å…¨é…ç½®
MAX_INPUT_LENGTH = 500
RATE_LIMIT = 10  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡è¯·æ±‚

# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_model():
    try:
        # ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–æ¨¡å‹å
        model_name = os.getenv("MODEL_NAME", "your-username/mae-intent-classifier")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def classify_intent(text, tokenizer, model):
    """ä½¿ç”¨RoBERTaæ¨¡å‹åˆ†ç±»æ„å›¾"""
    if not tokenizer or not model:
        return {"intent": "Error", "confidence": 0.0}
    
    # è¾“å…¥éªŒè¯
    if len(text) > MAX_INPUT_LENGTH:
        return {"intent": "Input too long", "confidence": 0.0}
    
    # é¢„å¤„ç†æ–‡æœ¬
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    # è·å–ç»“æœ
    intent_labels = [
        "Exploration and Reflection",
        "Feedback and Support", 
        "Goal Setting and Planning",
        "Problem Solving and Critical Thinking",
        "Understanding and Clarification"
    ]
    
    predicted_class = torch.argmax(predictions, dim=-1).item()
    confidence = predictions[0][predicted_class].item()
    intent = intent_labels[predicted_class]
    
    return {"intent": intent, "confidence": confidence}

def main():
    st.title("ğŸ“ MAE Intent Classifier")
    st.markdown("ä½¿ç”¨RoBERTaæ¨¡å‹è¿›è¡Œæ„å›¾åˆ†ç±»")
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        **å¦‚ä½•ä½¿ç”¨ï¼š**
        1. åœ¨ä¸‹æ–¹è¾“å…¥æ¡†ä¸­è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬
        2. ç‚¹å‡»"åˆ†ç±»"æŒ‰é’®
        3. æŸ¥çœ‹åˆ†ç±»ç»“æœå’Œç½®ä¿¡åº¦
        
        **æ”¯æŒçš„è¯­è¨€ï¼š** è‹±æ–‡
        
        **è¾“å…¥é™åˆ¶ï¼š** æœ€å¤š500ä¸ªå­—ç¬¦
        """)
    
    # åŠ è½½æ¨¡å‹
    with st.spinner("åŠ è½½æ¨¡å‹ä¸­..."):
        tokenizer, model = load_model()
    
    if tokenizer and model:
        st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # è¾“å…¥æ–‡æœ¬
        text_input = st.text_area(
            "è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬:",
            placeholder="ä¾‹å¦‚: I need help with my research direction...",
            height=100,
            max_chars=MAX_INPUT_LENGTH
        )
        
        # æ˜¾ç¤ºå­—ç¬¦è®¡æ•°
        st.caption(f"å­—ç¬¦æ•°: {len(text_input)}/{MAX_INPUT_LENGTH}")
        
        if st.button("åˆ†ç±»", type="primary"):
            if text_input.strip():
                with st.spinner("åˆ†ç±»ä¸­..."):
                    result = classify_intent(text_input, tokenizer, model)
                
                # æ˜¾ç¤ºç»“æœ
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("æ„å›¾", result["intent"])
                with col2:
                    st.metric("ç½®ä¿¡åº¦", f"{result['confidence']:.2%}")
                
                # å¯è§†åŒ–
                st.progress(result["confidence"])
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                with st.expander("ğŸ“Š è¯¦ç»†ä¿¡æ¯"):
                    st.json(result)
            else:
                st.warning("è¯·è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬")
    else:
        st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„")
    
    # é¡µè„š
    st.markdown("---")
    st.markdown("**MAE Intent Classifier** - åŸºäºRoBERTaçš„æ„å›¾åˆ†ç±»ç³»ç»Ÿ")

if __name__ == "__main__":
    main()
