"""
Llama 3.1 8B Instruct Integration for MAE Chatbot
é›†æˆ Llama 3.1 æ¨¡å‹ï¼Œæ›¿ä»£ OpenAI API
"""

import subprocess
import json
import time
from typing import Dict, Any, List
import re

class Llama3Manager:
    def __init__(self, model_name="llama3.1:8b"):
        self.model_name = model_name
        self.max_retries = 3
        self.timeout = 30
    
    def generate_response(self, prompt: str, system_prompt: str = None) -> str:
        """ä½¿ç”¨ Llama 3.1 ç”Ÿæˆå›å¤"""
        
        # æ„å»ºå®Œæ•´çš„æç¤º
        full_prompt = self._build_prompt(prompt, system_prompt)
        
        for attempt in range(self.max_retries):
            try:
                # è°ƒç”¨ Ollama
                result = subprocess.run([
                    "ollama", "run", self.model_name, full_prompt
                ], capture_output=True, text=True, timeout=self.timeout)
                
                if result.returncode == 0:
                    response = result.stdout.strip()
                    # æ¸…ç†å“åº”
                    response = self._clean_response(response, prompt)
                    return response
                else:
                    print(f"Ollama error (attempt {attempt + 1}): {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                print(f"Timeout (attempt {attempt + 1})")
            except Exception as e:
                print(f"Error (attempt {attempt + 1}): {e}")
            
            if attempt < self.max_retries - 1:
                time.sleep(2)  # ç­‰å¾…åé‡è¯•
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å›å¤
        return "I'm sorry, I'm having trouble responding right now. Could you please try again?"
    
    def _build_prompt(self, prompt: str, system_prompt: str = None) -> str:
        """æ„å»ºå®Œæ•´çš„æç¤º"""
        if system_prompt:
            return f"System: {system_prompt}\n\nUser: {prompt}\n\nAssistant:"
        else:
            return f"User: {prompt}\n\nAssistant:"
    
    def _clean_response(self, response: str, original_prompt: str) -> str:
        """æ¸…ç†å“åº”ï¼Œç§»é™¤é‡å¤çš„æç¤ºå†…å®¹"""
        # ç§»é™¤å¯èƒ½é‡å¤çš„æç¤ºéƒ¨åˆ†
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡åŒ…å«åŸå§‹æç¤ºçš„è¡Œ
            if original_prompt.lower() not in line.lower():
                # ç§»é™¤ "Assistant:" å‰ç¼€
                if line.startswith("Assistant:"):
                    line = line[10:].strip()
                cleaned_lines.append(line)
        
        cleaned_response = '\n'.join(cleaned_lines).strip()
        
        # å¦‚æœå“åº”å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å›å¤
        if len(cleaned_response) < 10:
            return "Thank you for your question. I'll do my best to help you."
        
        return cleaned_response

class Llama3ChatbotPipeline:
    def __init__(self, model_path="../pre-train/balanced_finetuned_model"):
        from models.intent_classifier import IntentClassifier
        from rag_system.vector_store import VectorStore
        from personas.persona_manager import PersonaManager
        
        self.classifier = IntentClassifier(model_path)
        self.vector_store = VectorStore()
        self.persona_manager = PersonaManager()
        self.llama3 = Llama3Manager()
    
    def process_message(self, user_input, persona="alpha"):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆä½¿ç”¨ Llama 3.1ï¼‰"""
        # 1. æ„å›¾åˆ†ç±»
        intent_result = self.classifier.classify(user_input)
        intent = intent_result["intent"]
        
        # 2. æ£€ç´¢ç›¸å…³çŸ¥è¯†
        context = self.vector_store.get_relevant_context(user_input, intent, persona)
        
        # 3. æ„å»ºç³»ç»Ÿæç¤º
        persona_info = self.persona_manager.get_persona(persona)
        persona_name = persona_info.get("name", "Advisor")
        persona_desc = persona_info.get("description", "")
        
        system_prompt = f"""You are a helpful MAE (Mechanical and Aerospace Engineering) peer advisor named {persona_name}.
Your characteristics: {persona_desc}
User's intent: {intent}
Relevant context: {context}

Please provide helpful, encouraging advice in the style of {persona_name}. Keep your response concise (2-3 sentences) and supportive."""
        
        # 4. ä½¿ç”¨ Llama 3.1 ç”Ÿæˆå›å¤
        answer = self.llama3.generate_response(user_input, system_prompt)
        
        return {
            "intent": intent,
            "context": context,
            "persona": persona,
            "answer": answer
        }
    
    def generate_student_reply(self, context, persona, advisor_intent=None):
        """ç”Ÿæˆå­¦ç”Ÿå›å¤ï¼ˆä½¿ç”¨ Llama 3.1ï¼‰"""
        from student_persona_manager import StudentPersonaManager
        
        spm = StudentPersonaManager()
        persona_info = spm.get_persona(persona)
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""You are a MAE student with the following characteristics:
Persona: {persona.upper()}
Description: {persona_info['description']}
Traits: {persona_info['traits']}
Help Seeking Behavior: {persona_info['help_seeking_behavior']}

The advisor just responded with intent: {advisor_intent if advisor_intent else 'Unknown'}

Generate a natural follow-up response as this student. Your response should:
1. Be consistent with your persona's traits and help-seeking behavior
2. Show appropriate engagement with the advisor's response
3. Be natural and conversational
4. Be 1-2 sentences long
5. Reflect your personality and confidence level"""
        
        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = f"Recent conversation:\n{context}\n\nGenerate a student response:"
        
        # ä½¿ç”¨ Llama 3.1 ç”Ÿæˆå›å¤
        return self.llama3.generate_response(user_prompt, system_prompt)

def test_llama3_integration():
    """æµ‹è¯• Llama 3.1 é›†æˆ"""
    print("ğŸ§ª Testing Llama 3.1 integration...")
    
    try:
        pipeline = Llama3ChatbotPipeline()
        
        # æµ‹è¯• advisor å›å¤
        print("Testing advisor response...")
        advisor_result = pipeline.process_message(
            "I'm struggling with my engineering classes. What should I do?",
            persona="alpha"
        )
        print(f"Advisor response: {advisor_result['answer']}")
        
        # æµ‹è¯•å­¦ç”Ÿå›å¤
        print("\nTesting student response...")
        student_reply = pipeline.generate_student_reply(
            context="advisor: You should seek help from professors and join study groups.\nstudent: I'm considering applying for a research position...",
            persona="alpha",
            advisor_intent="Feedback and Support"
        )
        print(f"Student response: {student_reply}")
        
        print("\nâœ… Llama 3.1 integration test successful!")
        return True
        
    except Exception as e:
        print(f"âŒ Integration test failed: {e}")
        return False

if __name__ == "__main__":
    test_llama3_integration()
